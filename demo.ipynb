{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with SSD\n",
    "### Here we demostrate detection on example images using SSD with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from box_utils import*\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.serialization import load_lua\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from ssd import build_ssd\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SSD300 in Test Phase\n",
    "1. Build the architecture, specifyingsize of the input image (300),\n",
    "    and number of object classes to score (21 for VOC dataset)\n",
    "2. Next we load pretrained weights on the VOC 2007 dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-30939af3d083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ssd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# initialize SSD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg16_layers_fc_reduced.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg16_layers_fc_reduced.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#net.load_weights('weights/ssd_300_voc0712.pkl')  # load weights by specifying location of either .pkl or.t7 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amdegroot/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[1;32m    315\u001b[0m         \u001b[0mown_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mown_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise KeyError('unexpected key \"{}\" in state_dict'\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "net = build_ssd('test', 300, 21)    # initialize SSD\n",
    "net.vgg.load_state_dict('vgg16_layers_fc_reduced.pth')\n",
    "\n",
    "net.vgg\n",
    "#net.load_weights('weights/ssd_300_voc0712.pkl')  # load weights by specifying location of either .pkl or.t7 file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image \n",
    "### Here we just load an example image provided in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = Image.open('./data/example.jpg').convert('RGB') \n",
    "# Using the torchvision package, we can create a Compose of multiple \n",
    "# built-in transorm ops to apply \n",
    "t = transforms.Compose([\n",
    "        transforms.Scale(300),\n",
    "        transforms.CenterCrop(300),\n",
    "    ])\n",
    "# View the transformed input image \n",
    "t(image) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD Forward Pass\n",
    "### Now just wrap the image in a Variable so it is recognized by PyTorch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = img.float()\n",
    "x = Variable(x) # wrap tensor in Variable\n",
    "y = net(x)      # forward pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Parse the Detections\n",
    "Filter outputs with confidence scores lower than a threshold \n",
    "Here we choose 60% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data import VOC_CLASSES as labelmap\n",
    "import numpy as np\n",
    "\n",
    "def get_labelname(labelmap, top_label_indices):\n",
    "    return [labelmap[int(l)] for l in top_label_indices]\n",
    "\n",
    "detections = y.data.numpy()\n",
    "# Parse the outputs.\n",
    "det_label = detections[0,:,1]\n",
    "det_conf = detections[0,:,2]\n",
    "det_xmin = detections[0,:,3]\n",
    "det_ymin = detections[0,:,4]\n",
    "det_xmax = detections[0,:,5]\n",
    "det_ymax = detections[0,:,6]\n",
    "\n",
    "# Get detections with confidence higher than 0.6.\n",
    "top_indices = [i for i, conf in enumerate(det_conf) if conf >= 0.6]\n",
    "\n",
    "top_conf = det_conf[top_indices]\n",
    "top_label_indices = det_label[top_indices]\n",
    "top_labels = get_labelname(labelmap, top_label_indices)\n",
    "top_xmin = det_xmin[top_indices]\n",
    "top_ymin = det_ymin[top_indices]\n",
    "top_xmax = det_xmax[top_indices]\n",
    "top_ymax = det_ymax[top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "plt.imshow(image)  # plot the image for matpltlib\n",
    "currentAxis = plt.gca()\n",
    "# scale each detection back up to the image\n",
    "for i in range(top_conf.shape[0]):\n",
    "    xmin = int(round(top_xmin[i] * image.size[0]))\n",
    "    ymin = int(round(top_ymin[i] * image.size[1]))\n",
    "    xmax = int(round(top_xmax[i] * image.size[0]))\n",
    "    ymax = int(round(top_ymax[i] * image.size[1]))\n",
    "    score = top_conf[i]\n",
    "    label = int(top_label_indices[i])\n",
    "    label_name = top_labels[i]\n",
    "    display_txt = '%s: %.2f'%(label_name, score)\n",
    "    coords = (xmin, ymin), xmax-xmin+1, ymax-ymin+1\n",
    "    color = colors[label]\n",
    "    currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "    currentAxis.text(xmin, ymin, display_txt, bbox={'facecolor':color, 'alpha':0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()  # display label, score, and bounding boxes!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
