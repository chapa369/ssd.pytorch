{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from data import VOCroot\n",
    "from data import VOC_CLASSES as labelmap\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "from data import AnnotationTransform, VOCDetection, base_transform\n",
    "from timeit import default_timer as timer\n",
    "import argparse\n",
    "import numpy as np\n",
    "from ssd import build_ssd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_net(save_folder, net, cuda, valset, transform, top_k, thresh):\n",
    "    # dump predictions and assoc. ground truth to text file for now\n",
    "    filename = save_folder+'test1.txt'\n",
    "    num_images = len(valset)\n",
    "    recs = {}\n",
    "    class_recs = {}\n",
    "    npos = 0\n",
    "\n",
    "    for i in range(num_images):\n",
    "        print('Evaluating image {:d}/{:d}....'.format(i+1, num_images))\n",
    "        img = valset.pull_image(i)\n",
    "        anno = valset.pull_anno(i)\n",
    "        print(anno)\n",
    "        anno = torch.Tensor(anno).long()\n",
    "        boxes, labels = torch.split(anno, 4, 1) # maybe also return img id\n",
    "        if(boxes.dim() == 1):\n",
    "            boxes.unsqueeze_(0)\n",
    "\n",
    "        det = [False] * boxes.size(0) # detected in given image\n",
    "        x = Variable(transform(img).unsqueeze_(0))\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        y = net(x)      # forward pass\n",
    "        detections = y.data\n",
    "        # scale each detection back up to the image\n",
    "        scale = torch.Tensor([img.size[0],img.size[1],img.size[0],img.size[1]])\n",
    "        pred_num = 0\n",
    "        # for each class\n",
    "        for i in range(detections.size(1)):\n",
    "            j = 0\n",
    "            # filter out detections that scored less than 0.01\n",
    "            while detections[0,i,j,0] >= 0.01:\n",
    "                pt = (detections[0,i,j,1:]*scale)\n",
    "                pred_num+=1\n",
    "                \n",
    "            # compute overlaps\n",
    "            # if overlap > threshold (0.5)\n",
    "                # if not found => true positive, mark as found\n",
    "                # else => duplicate... false positive\n",
    "            # else => false positive\n",
    "    # for each class, calc\n",
    "        # precision (fp / tp+fp)\n",
    "        # recall (tp+fp / #gt) => gt = tp + fn\n",
    "    # calc avg prec for each class\n",
    "    # mAP = mean of APs for all classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model!\n",
      "Evaluating image 1/2510....\n",
      "[[262.0, 210.0, 323.0, 338.0, 8], [164.0, 263.0, 252.0, 371.0, 8], [240.0, 193.0, 294.0, 298.0, 8]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5f7063abd83e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvalset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVOCDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAnnotationTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m104\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m117\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-c6f362ce0e78>\u001b[0m in \u001b[0;36mtest_net\u001b[0;34m(save_folder, net, cuda, valset, transform, top_k, thresh)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mpred_num\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amdegroot/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0m__rmul__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = 'weights/ssd_300_VOC0712.pth'\n",
    "\n",
    "# load net\n",
    "net = build_ssd('test', 300, 21)    # initialize SSD\n",
    "net.load_state_dict(torch.load(trained_model))\n",
    "net.eval()\n",
    "print('Finished loading model!')\n",
    "# load data\n",
    "valset = VOCDetection(VOCroot, 'val', None, AnnotationTransform())\n",
    "# evaluation\n",
    "test_net('eval/', net, False, valset, base_transform(net.size,(104,117,123)), 5, thresh=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = build_ssd_v1('test', 300, 21)    # initialize SSD\n",
    "# net.load_weights('weights/ssd_300_VOC0712.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1,3,300,300)\n",
    "x = Variable(x)     # wrap tensor in Variable\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([[1,2,3]])\n",
    "print(x)\n",
    "idx = torch.LongTensor([[0,2]])\n",
    "x.gather(1,idx)[0,:] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.scatter_(1,idx,torch.Tensor([[0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Demo\n",
    "\n",
    "Here we demonstrate framework testing by simulating an SVM classifier and its  \n",
    "resulting classification accuracy after an attack from the simple_optimize adversary.\n",
    "\n",
    "## Initial Learner Training\n",
    "- choose an initial learner model,\n",
    "- load the instances for training,\n",
    "- and wrap the two with the Classifier wrapper. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pre-process data and randomly partition\n",
    "instances = input.load_instances('./data_reader/data/test/100_instance_debug')\n",
    "random.shuffle(instances)\n",
    "instances2 = instances[:60]\n",
    "instances3 = instances[60:]\n",
    "# print(sparsify(instances)[1].toarray())\n",
    "\n",
    "\n",
    "# initialize sklearn model\n",
    "learning_model = svm.SVC(probability=True, kernel='linear')\n",
    "\n",
    "# initialize and train RobustLearner\n",
    "clf2 = learner.SimpleLearner(learning_model, instances2)\n",
    "clf2.train()\n",
    "\n",
    "# produce simple metrics\n",
    "y_predict = clf2.predict(instances3)\n",
    "y_true = sparsify(instances3)[0]\n",
    "print(y_predict)\n",
    "print(y_true)\n",
    "score = metrics.accuracy_score(y_true,y_predict)\n",
    "print(\"score = \"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pre-process data and randomly partition\n",
    "instances = input.load_instances('./data_reader/data/test/100_instance_debug')\n",
    "random.shuffle(instances)\n",
    "instances2 = instances[:60]\n",
    "instances3 = instances[60:]\n",
    "# print(sparsify(instances)[1].toarray())\n",
    "\n",
    "\n",
    "# initialize and train RobustLearner\n",
    "clf2 = learner.SVMFreeRange({'c_f': 0.5}, instances2)\n",
    "clf2.train()\n",
    "\n",
    "# produce simple metrics\n",
    "y_predict = clf2.predict(instances3)\n",
    "y_true = sparsify(instances3)[0]\n",
    "print(y_predict)\n",
    "print(y_true)\n",
    "score = metrics.accuracy_score(y_true,y_predict)\n",
    "print(\"score = \"+str(score))\n",
    "\n",
    "# wgt = clf2.decision_function()[0].tolist()[0]\n",
    "# print(wgt)\n",
    "# yaxis = [i for i in range(clf2.num_features)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pre-process data and randomly partition\n",
    "instances = input.load_instances('./data_reader/data/test/100_instance_debug')\n",
    "random.shuffle(instances)\n",
    "instances2 = instances[:60]\n",
    "instances3 = instances[60:]\n",
    "# print(sparsify(instances)[1].toarray())\n",
    "\n",
    "# initialize and train RobustLearner\n",
    "clf2 = learner.SVMRestrained({'c_delta': 0.5}, instances2)\n",
    "clf2.train()\n",
    "\n",
    "# produce simple metrics\n",
    "y_predict = clf2.predict(instances3)\n",
    "y_true = sparsify(instances3)[0]\n",
    "print(y_predict)\n",
    "print(y_true)\n",
    "score = metrics.accuracy_score(y_true,y_predict)\n",
    "print(\"score = \"+str(score))\n",
    "\n",
    "# wgt = clf2.decision_function()[0].tolist()[0]\n",
    "# print(wgt)\n",
    "# yaxis = [i for i in range(clf2.num_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# plt.plot(yaxis, wgt)\n",
    "# plt.axis([0,1000,-0.00000000005,0.000000000005])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.train() # train the initial learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversary Attack \n",
    "Now we choose an adversary attack strategy to transform the \n",
    "original instances, here we choose simple_optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize attacker from the ad package\n",
    "adv = ad.SimpleOptimize(lambda_val=-100, max_change = 65, learner = clf)\n",
    "# perform the attack on the data to transform instances\n",
    "instances_post_attack = adv.attack(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph results of pre attack vs. post attack learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# obtain metrics of classification performance both pre- and post- attack\n",
    "metrics = EvasionMetrics(clf, instances, instances_post_attack)\n",
    "preAccuracy = metrics.getAccuracy(True)\n",
    "postAccuracy = metrics.getAccuracy(False)\n",
    "print (\"Pre-attack Accuracy (naive classifier): \" + str(preAccuracy))\n",
    "print (\"Post-attack Accuracy: (naive classifier): \" + str(postAccuracy))\n",
    "\n",
    "# plot ROC curve of naive classifier performance after attack\n",
    "metrics.plotROC(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner Response\n",
    "Here we wrap a defense strategy for the learner to exercise\n",
    "during the next round of training to become more robust. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "instances2 = input.load_instances('./data_reader/data/test/100_instance_debug') # load instances\n",
    "learning_model2 = svm.SVC(probability=True, kernel='linear')\n",
    "clf2 = Classifier(learning_model2, instances2, [\"retraining\", adv.clone()]) # could also just pass blank adv instance\n",
    "clf2.train() # train again with defense strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversary Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we launch the attack on the robust classifier \n",
    "adv = ad.SimpleOptimize(lambda_val=-100, max_change = 65, learner = clf2)\n",
    "instances_post_attack2 = adv.attack(instances2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display metrics after attack on the robust classifier\n",
    "Here we see the improvements made in accuracy as a result \n",
    "of the chosen defense strategy.  Clearly, our learner has \n",
    "become more robust to adversary attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# obtain metrics of classification performance both pre- and post- attack\n",
    "metrics2 = EvasionMetrics(clf2, instances, instances_post_attack2)\n",
    "preAccuracy2 = metrics2.getAccuracy(True)\n",
    "postAccuracy2 = metrics2.getAccuracy(False)\n",
    "print (\"Pre-attack Accuracy (robust classifier): \" + str(preAccuracy2))\n",
    "print (\"Post-attack Accuracy: (robust classifier): \" + str(postAccuracy2))\n",
    "\n",
    "metrics2.plotROC(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
